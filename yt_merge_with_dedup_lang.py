import pandas as pd
import glob
import re
import os
from langdetect import detect, LangDetectException

def contains_timestamp(text):
    """
    í…ìŠ¤íŠ¸ì— íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    0:00, 00:00, 12:34, 1:23:45 ë“± ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    # NaNì´ë‚˜ None ê°’ ì²˜ë¦¬
    if pd.isna(text) or not isinstance(text, str):
        return False
    
    pattern = r"(?<!\d)(?:[0-5]?\d:)?[0-5]?\d:[0-5]\d(?!\d)"
    return re.search(pattern, text) is not None

def extract_timestamps(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # NaNì´ë‚˜ None ê°’ ì²˜ë¦¬
    if pd.isna(text) or not isinstance(text, str):
        return []
    
    pattern = r"(?<!\d)(?:[0-5]?\d:)?[0-5]?\d:[0-5]\d(?!\d)"
    timestamps = re.findall(pattern, text)
    return timestamps

def detect_language(text):
    """
    í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    ê°ì§€ ì‹¤íŒ¨ ì‹œ 'unknown'ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # NaNì´ë‚˜ None ê°’ ì²˜ë¦¬
        if pd.isna(text) or not isinstance(text, str):
            return 'unknown'
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì§€ ë¶ˆê°€
        if len(text.strip()) < 3:
            return 'unknown'
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì–¸ì–´ ê°ì§€
        clean_text = re.sub(r'[0-9]+:[0-9]+', '', text)  # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
        clean_text = re.sub(r'[^\w\s]', ' ', clean_text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
        clean_text = clean_text.strip()
        
        if len(clean_text) < 3:
            return 'unknown'
            
        language = detect(clean_text)
        return language
    except LangDetectException:
        return 'unknown'
    except Exception:
        return 'unknown'

def remove_duplicates(df):
    """
    ì¤‘ë³µëœ ëŒ“ê¸€ì„ ì œê±°í•©ë‹ˆë‹¤.
    comment ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µì„ ì°¾ê³ , likesê°€ ë†’ì€ ê²ƒì„ ìœ ì§€í•©ë‹ˆë‹¤.
    """
    # comment ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (likesê°€ ë†’ì€ ê²ƒ ìœ ì§€)
    df_no_duplicates = df.sort_values('likes', ascending=False).drop_duplicates(subset=['comment'], keep='first')
    
    return df_no_duplicates

def filter_by_language(df, target_languages=None):
    """
    íŠ¹ì • ì–¸ì–´ì˜ ëŒ“ê¸€ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
    target_languagesê°€ Noneì´ë©´ ëª¨ë“  ì–¸ì–´ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
    """
    if target_languages is None or 'language' not in df.columns:
        return df
    
    # ì§€ì •ëœ ì–¸ì–´ë§Œ ìœ ì§€
    df_filtered = df[df['language'].isin(target_languages)]
    
    return df_filtered


def load_video_mapping():
    """
    video_id_mapping.csv íŒŒì¼ì„ ì½ì–´ì„œ ë¹„ë””ì˜¤ IDì™€ ì±„ë„ëª…, ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        mapping_df = pd.read_csv(os.path.join("csv", "video_id_mapping.csv"))
        # video_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
        video_mapping = {}
        for _, row in mapping_df.iterrows():
            if pd.notna(row['video_id']):
                video_mapping[row['video_id']] = {
                    'channel_name': row['channel_name'],
                    'category': row['category']
                }
        return video_mapping
    except FileNotFoundError:
        print("âš ï¸ csv/video_id_mapping.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {}
    except Exception as e:
        print(f"âš ï¸ csv/video_id_mapping.csv íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}. ê¸°ë³¸ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {}

def merge_filtered_files_with_dedup_lang(target_languages=None, top_n=20):
    """
    í•„í„°ë§ëœ íŒŒì¼ë“¤ì„ ë³‘í•©í•˜ê³  ì¤‘ë³µ ì œê±°, ì–¸ì–´ ì‹ë³„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        target_languages (list): í•„í„°ë§í•  ì–¸ì–´ ëª©ë¡ (ì˜ˆ: ['ko', 'en']). Noneì´ë©´ ëª¨ë“  ì–¸ì–´ ìœ ì§€
        top_n (int): ê° íŒŒì¼ì—ì„œ ì¶”ì¶œí•  ìƒìœ„ ëŒ“ê¸€ ìˆ˜
    """
    # csv/video_id_mapping.csvì—ì„œ ì •ë³´ ë¡œë“œ
    video_mapping = load_video_mapping()
    
    # Comments í´ë”ì—ì„œ {video_id}_comments.csv íŒŒì¼ë“¤ ì°¾ê¸°
    comments_dir = "Comments"
    if not os.path.exists(comments_dir):
        print(f"âŒ {comments_dir} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # comments í´ë”ì˜ ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸°
    filtered_files = glob.glob(os.path.join(comments_dir, "*_comments.csv"))
    
    print(f"ğŸ“ ì°¾ì€ íŒŒì¼ë“¤: {filtered_files}")
    
    # í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜ë“¤
    total_stats = {
        'original_comments': 0,
        'timestamp_comments': 0,
        'expanded_comments': 0,
        'after_dedup': 0,
        'after_lang_filter': 0,
        'final_selected': 0,
        'duplicates_removed': 0,
        'lang_filtered': 0
    }
    
    all_data = []
    
    for file_path in filtered_files:
        # íŒŒì¼ëª…ì—ì„œ video_id ì¶”ì¶œ (ì˜ˆ: "myO8fxhDRW0_comments.csv" -> "myO8fxhDRW0")
        filename = os.path.basename(file_path)
        video_id = filename.replace("_comments.csv", "")
        
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(file_path)
        
        # í†µê³„ ëˆ„ì 
        total_stats['original_comments'] += len(df)
        
        # video_id ì»¬ëŸ¼ ì¶”ê°€
        df['video_id'] = video_id
        
        # ì±„ë„ëª…ê³¼ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
        if video_id in video_mapping:
            df['channel_name'] = video_mapping[video_id]['channel_name']
            df['category'] = video_mapping[video_id]['category']
        else:
            print(f"âš ï¸ {video_id}ì— ëŒ€í•œ ë§¤í•‘ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            df['channel_name'] = 'Unknown'
            df['category'] = 'Unknown'
        
        # comment ì»¬ëŸ¼ ì²˜ë¦¬ (commentê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì»¬ëŸ¼ ì°¾ê¸°)
        if 'comment' not in df.columns:
            # username, time, likes, video_id, channel_name, categoryë¥¼ ì œì™¸í•œ ì»¬ëŸ¼ì„ commentë¡œ ì‚¬ìš©
            available_cols = [col for col in df.columns if col not in ['username', 'time', 'likes', 'video_id', 'channel_name', 'category']]
            if available_cols:
                df['comment'] = df[available_cols[0]]
        
        # ëŒ“ê¸€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
        df['timestamp'] = df['comment'].apply(extract_timestamps)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ëŒ“ê¸€ë§Œ í•„í„°ë§
        df_with_timestamp = df[df['timestamp'].apply(lambda x: len(x) > 0)].copy()
        total_stats['timestamp_comments'] += len(df_with_timestamp)
        
        if len(df_with_timestamp) > 0:
            # timestampê°€ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ê°ê°ì„ ë³„ë„ í–‰ìœ¼ë¡œ ë¶„ë¦¬
            expanded_rows = []
            
            for idx, row in df_with_timestamp.iterrows():
                timestamps = row['timestamp']
                
                # ê° timestampì— ëŒ€í•´ ë³„ë„ í–‰ ìƒì„±
                for timestamp in timestamps:
                    new_row = row.copy()
                    new_row['timestamp'] = [timestamp]  # ë‹¨ì¼ timestampë¡œ ë³€ê²½
                    expanded_rows.append(new_row)
            
            # ë¶„ë¦¬ëœ ë°ì´í„°ë¡œ ìƒˆë¡œìš´ DataFrame ìƒì„±
            df_expanded = pd.DataFrame(expanded_rows)
            total_stats['expanded_comments'] += len(df_expanded)
            
            # ìˆ«ì ë³€í™˜
            df_expanded["likes"] = pd.to_numeric(df_expanded["likes"], errors="coerce")
            
            # ì¤‘ë³µ ì œê±°
            before_dedup = len(df_expanded)
            df_expanded = remove_duplicates(df_expanded)
            after_dedup = len(df_expanded)
            total_stats['duplicates_removed'] += (before_dedup - after_dedup)
            total_stats['after_dedup'] += after_dedup
            
            # ì–¸ì–´ ì‹ë³„ ì¶”ê°€
            df_expanded['language'] = df_expanded['comment'].apply(detect_language)
            
            # ì–¸ì–´ í•„í„°ë§ (ì˜µì…˜)
            if target_languages:
                before_lang_filter = len(df_expanded)
                df_expanded = filter_by_language(df_expanded, target_languages)
                after_lang_filter = len(df_expanded)
                total_stats['lang_filtered'] += (before_lang_filter - after_lang_filter)
                total_stats['after_lang_filter'] += after_lang_filter
            else:
                total_stats['after_lang_filter'] += len(df_expanded)
            
            # ìƒìœ„ Nê°œ ëŒ“ê¸€ë§Œ ì¶”ì¶œ
            df_top = df_expanded.sort_values(by="likes", ascending=False).head(top_n).copy()
            total_stats['final_selected'] += len(df_top)
            
            # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
            columns_to_remove = ["username", "time", "filename"]
            for col in columns_to_remove:
                if col in df_top.columns:
                    df_top = df_top.drop(col, axis=1)
            
            all_data.append(df_top)
    
    # ëª¨ë“  ë°ì´í„° í•©ì¹˜ê¸°
    if all_data:
        merged_df = pd.concat(all_data, ignore_index=True)
        print(f"\nğŸ¯ ìµœì¢… í•©ì³ì§„ ë°ì´í„°: {len(merged_df)} í–‰")
        
        # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
        final_columns = []
        for col in ["video_id", "channel_name", "category", "timestamp", "comment", "language", "likes"]:
            if col in merged_df.columns:
                final_columns.append(col)
        
        merged_df = merged_df[final_columns]
        
        # ì „ì²´ ë°ì´í„° ì–¸ì–´ í†µê³„
        if 'language' in merged_df.columns:
            total_language_counts = merged_df['language'].value_counts()
            print(f"ğŸ“Š ì „ì²´ ì–¸ì–´ë³„ ëŒ“ê¸€ ìˆ˜: {dict(total_language_counts)}")
        
        # ì±„ë„ë³„ í†µê³„
        if 'channel_name' in merged_df.columns:
            channel_counts = merged_df['channel_name'].value_counts()
            print(f"ğŸ“º ì±„ë„ë³„ ëŒ“ê¸€ ìˆ˜: {dict(channel_counts)}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        if 'category' in merged_df.columns:
            category_counts = merged_df['category'].value_counts()
            print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë³„ ëŒ“ê¸€ ìˆ˜: {dict(category_counts)}")
        
        # ê²°ê³¼ ì €ì¥
        output_file = os.path.join("csv", "merged_filtered_comments_with_dedup_lang.csv")
        os.makedirs("csv", exist_ok=True)
        merged_df.to_csv(output_file, index=False)
        print(f"ğŸ’¾ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì „ì²´ ì²˜ë¦¬ í†µê³„ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š ì „ì²´ ì²˜ë¦¬ í†µê³„ ìš”ì•½")
        print("="*60)
        print(f"ğŸ“ ì›ë³¸ ëŒ“ê¸€ ìˆ˜: {total_stats['original_comments']:,}ê°œ")
        print(f"â° íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ëŒ“ê¸€: {total_stats['timestamp_comments']:,}ê°œ")
        print(f"ğŸ”„ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ë¦¬ í›„: {total_stats['expanded_comments']:,}ê°œ")
        print(f"ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {total_stats['duplicates_removed']:,}ê°œ ì œê±°")
        print(f"ğŸŒ ì–¸ì–´ í•„í„°ë§: {total_stats['lang_filtered']:,}ê°œ ì œê±°")
        print(f"ğŸ“Š ìƒìœ„ {top_n}ê°œ ì„ ë³„: {total_stats['final_selected']:,}ê°œ")
        print(f"ğŸ† ìµœì¢… ê²°ê³¼: {total_stats['final_selected']:,}ê°œ")
        print("="*60)
        
        # ì²« ëª‡ í–‰ ì¶œë ¥
        print("\nğŸ“‹ ì²« 5í–‰:")
        print(merged_df.head())
        
        return merged_df
    else:
        print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ì˜ì–´ë§Œ í•„í„°ë§í•˜ì—¬ ì‹¤í–‰
    """
    print("ğŸš€ YouTube ëŒ“ê¸€ ë³‘í•© ë° ì²˜ë¦¬ ì‹œì‘ (ì˜ì–´ë§Œ)")
    print("=" * 50)
    
    # ì˜ì–´ë§Œ í•„í„°ë§ + ì¤‘ë³µ ì œê±°
    print("\nğŸ“ ì˜ì–´ ëŒ“ê¸€ë§Œ í•„í„°ë§ + ì¤‘ë³µ ì œê±°")
    merge_filtered_files_with_dedup_lang(
        target_languages=['en'], 
        top_n=20
    )

if __name__ == "__main__":
    main()
