import os
import json

# Project root is parent of scripts/
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(_SCRIPT_DIR)

import torch
import soundfile as sf
from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info
import subprocess
from tqdm import tqdm
import cv2
import numpy as np
from PIL import Image
from typing import List, Dict, Any, Tuple
from pathlib import Path
import warnings
import pandas as pd
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import gc

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# root loggerì˜ WARNING ë ˆë²¨ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
import logging
logging.getLogger().setLevel(logging.ERROR)
logging.getLogger("root").setLevel(logging.ERROR)


class OptimizedMomentQueryGenerator:
    def __init__(self, 
                 model_path: str = "Qwen/Qwen2.5-Omni-7B", 
                 segment_duration: int = 6,
                 num_segments: int = 3,
                 max_batch_size: int = 3,
                 max_new_tokens: int = 200,
                 video_fps: int = 24,
                 ffmpeg_preset: str = "ultrafast",
                 ffmpeg_crf: int = 28,
                 use_model_compile: bool = True,
                 use_flash_attention: bool = True,
                 torch_dtype_str: str = "float16",
                 max_workers_segments: int = 3,
                 max_workers_analysis: int = 2,
                 ffmpeg_threads: int = None,
                 device_map: str = "auto",
                 max_memory: Dict[str, str] = None):
        """ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ìš© Qwen 2.5 Omni ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # ì„¤ì • ì €ì¥
        self.model_path = model_path
        self.segment_duration = segment_duration
        self.num_segments = num_segments
        self.max_batch_size = max_batch_size
        self.max_new_tokens = max_new_tokens
        self.video_fps = video_fps
        self.ffmpeg_preset = ffmpeg_preset
        self.ffmpeg_crf = ffmpeg_crf
        self.use_model_compile = use_model_compile
        self.use_flash_attention = use_flash_attention
        self.USE_AUDIO_IN_VIDEO = True
        self.max_workers_segments = max_workers_segments
        self.max_workers_analysis = max_workers_analysis
        self.ffmpeg_threads = ffmpeg_threads
        self.device_map = device_map
        self.max_memory = max_memory
        
        # ìë™ ê³„ì‚°ë˜ëŠ” ê°’ë“¤
        self.total_duration = self.num_segments * self.segment_duration
        self.half_duration = self.total_duration // 2
        
        # torch dtype ì„¤ì •
        torch_dtype_map = {
            "float16": torch.float16,
            "float32": torch.float32,
            "bfloat16": torch.bfloat16,
            "auto": "auto"
        }
        self.torch_dtype = torch_dtype_map.get(torch_dtype_str, torch.float16)
        
        # GPU ë©”ëª¨ë¦¬ ìµœì í™”
        torch.cuda.empty_cache()
        
        # ëª¨ë¸ ë¡œë“œ ì„¤ì • ì¤€ë¹„
        model_kwargs = {
            "torch_dtype": self.torch_dtype,
            "device_map": self.device_map,
            "low_cpu_mem_usage": True,
        }
        
        if self.max_memory:
            model_kwargs["max_memory"] = self.max_memory
        
        # Flash Attention ì„¤ì •
        if self.use_flash_attention:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
            except:
                print("Flash Attention 2 ì‚¬ìš© ë¶ˆê°€, ê¸°ë³¸ ì–´í…ì…˜ ì‚¬ìš©")
        
        # ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
            self.model_path, **model_kwargs
        )
        self.processor = Qwen2_5OmniProcessor.from_pretrained(self.model_path)
        
        # ëª¨ë¸ ì»´íŒŒì¼ (ì˜µì…˜)
        if self.use_model_compile:
            try:
                print("ëª¨ë¸ ì»´íŒŒì¼ ì¤‘... (ì²« ì‹¤í–‰ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                self.model = torch.compile(self.model, mode="reduce-overhead")
                print("âœ“ ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ")
            except Exception as e:
                print(f"ëª¨ë¸ ì»´íŒŒì¼ ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
        
        # ëª¨ë¸ì„ eval ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬ ê´€ë ¨ ì„¤ì •
        self.video_dir = Path("/home/elicer/yt_dataset/youtube_videos")
        self.segments_dir = Path(f"optimized_test_video_segments_{self.total_duration}s")
        self.segments_dir.mkdir(exist_ok=True)
        
        # ìµœì í™”ëœ ìƒì„± íŒŒë¼ë¯¸í„°
        self.generation_config = {
            "max_new_tokens": self.max_new_tokens,
            "do_sample": False,     # ê·¸ë¦¬ë”” ë””ì½”ë”©ìœ¼ë¡œ ì†ë„ í–¥ìƒ
            "repetition_penalty": 1.1,
            "pad_token_id": self.processor.tokenizer.eos_token_id,
            "use_cache": True,      # KV ìºì‹œ ì‚¬ìš©
        }
        
        print("ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        print(f"ëª¨ë¸ ê²½ë¡œ: {self.model_path}")
        print(f"ì„¸ê·¸ë¨¼íŠ¸ êµ¬ì„±: {self.num_segments}ê°œ Ã— {self.segment_duration}ì´ˆ = {self.total_duration}ì´ˆ")
        print(f"ì‹¤ì œ ë¶„ì„ ë²”ìœ„: ì¤‘ì‹¬ Â±{self.half_duration}ì´ˆ (ì´ {self.total_duration}ì´ˆ)")
        print(f"ìµœëŒ€ ë°°ì¹˜ í¬ê¸°: {self.max_batch_size}")
        print(f"ìµœëŒ€ í† í° ìˆ˜: {self.max_new_tokens}")
        print(f"ë¹„ë””ì˜¤ FPS: {self.video_fps}")
        print(f"FFmpeg ì„¤ì •: {self.ffmpeg_preset} í”„ë¦¬ì…‹, CRF {self.ffmpeg_crf}")
        if self.ffmpeg_threads:
            print(f"FFmpeg ìŠ¤ë ˆë“œ: {self.ffmpeg_threads}")
        print(f"ëª¨ë¸ ì»´íŒŒì¼: {'ON' if self.use_model_compile else 'OFF'}")
        print(f"Flash Attention: {'ON' if self.use_flash_attention else 'OFF'}")
        print(f"Torch dtype: {self.torch_dtype}")
        print(f"ë³‘ë ¬ ì²˜ë¦¬: ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± {self.max_workers_segments}ê°œ, ë¶„ì„ {self.max_workers_analysis}ê°œ")
        print(f"ë””ë°”ì´ìŠ¤ ë§¤í•‘: {self.device_map}")
        if self.max_memory:
            print(f"GPU ë©”ëª¨ë¦¬ ì œí•œ: {self.max_memory}")
        print(f"âš¡ ì´ ë¶„ì„ ì‹œê°„: {self.total_duration}ì´ˆ")
    
    def find_video_file(self, video_id: str, channel_name: str = None, category: str = None) -> str:
        """ê¸°ì¡´ ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡° ì§€ì›)"""
        # ê°„ë‹¨í•œ íŒŒì¼ ìºì‹œ
        if not hasattr(self, '_video_cache'):
            self._video_cache = {}
        
        cache_key = f"{video_id}_{channel_name}_{category}"
        if cache_key in self._video_cache:
            return self._video_cache[cache_key]
        
        extensions = ['.mp4', '.mkv', '.avi', '.mov', '.webm']
        
        # 1. ìƒˆë¡œìš´ ê²½ë¡œ êµ¬ì¡° ì‹œë„: /youtube_videos/{category}/{channel_name}/{video_id}.ext
        if channel_name and category:
            from pathlib import Path
            base_dir = Path(self.video_dir).parent / "youtube_videos"
            new_structure_dir = base_dir / category / channel_name
            
            if new_structure_dir.exists():
                for ext in extensions:
                    video_path = new_structure_dir / f"{video_id}{ext}"
                    if video_path.exists():
                        self._video_cache[cache_key] = str(video_path)
                        return str(video_path)
                
                # video_idê°€ í¬í•¨ëœ íŒŒì¼ ì°¾ê¸°
                for video_file in new_structure_dir.glob(f"*{video_id}*"):
                    if video_file.suffix in extensions:
                        self._video_cache[cache_key] = str(video_file)
                        return str(video_file)
        
        # 2. ê¸°ì¡´ ê²½ë¡œ êµ¬ì¡° ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
        for ext in extensions:
            video_path = self.video_dir / f"{video_id}{ext}"
            if video_path.exists():
                self._video_cache[cache_key] = str(video_path)
                return str(video_path)
        
        # video_idê°€ í¬í•¨ëœ íŒŒì¼ ì°¾ê¸°
        for video_file in self.video_dir.glob(f"*{video_id}*"):
            if video_file.suffix in extensions:
                self._video_cache[cache_key] = str(video_file)
                return str(video_file)
        
        self._video_cache[cache_key] = None
        return None
    
    def check_video_compatibility(self, video_path: str) -> bool:
        """ë¹„ë””ì˜¤ íŒŒì¼ì˜ í˜¸í™˜ì„± í™•ì¸ (ìºì‹œ ì¶”ê°€)"""
        if not hasattr(self, '_compatibility_cache'):
            self._compatibility_cache = {}
        
        if video_path in self._compatibility_cache:
            return self._compatibility_cache[video_path]
        
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-print_format", "json", 
                "-show_streams", video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                streams_info = json.loads(result.stdout)
                
                for stream in streams_info['streams']:
                    if stream['codec_type'] == 'video':
                        is_compatible = (stream.get('codec_name') == 'h264' and 
                                       stream.get('pix_fmt') == 'yuv420p')
                        self._compatibility_cache[video_path] = is_compatible
                        return is_compatible
                        
            self._compatibility_cache[video_path] = False
            return False
        except Exception as e:
            self._compatibility_cache[video_path] = False
            return False
    
    def parse_timestamp(self, timestamp_str: str) -> List[float]:
        """timestamp ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜ (h:mm:ss, mm:ss, ss ëª¨ë‘ ì§€ì›)"""
        try:
            import ast
            if timestamp_str.startswith('[') and timestamp_str.endswith(']'):
                timestamp_list = ast.literal_eval(timestamp_str)
            else:
                timestamp_list = [timestamp_str]
            
            seconds_list = []
            for ts in timestamp_list:
                if ':' in ts:
                    parts = ts.split(':')
                    if len(parts) == 3:
                        # h:mm:ss í˜•ì‹
                        hours, minutes, seconds = parts
                        total_seconds = int(hours) * 3600 + int(minutes) * 60 + int(seconds)
                        seconds_list.append(float(total_seconds))
                    elif len(parts) == 2:
                        # mm:ss í˜•ì‹
                        minutes, seconds = parts
                        total_seconds = int(minutes) * 60 + int(seconds)
                        seconds_list.append(float(total_seconds))
                    else:
                        # ì˜ëª»ëœ í˜•ì‹
                        seconds_list.append(float(parts[0]))
                else:
                    # ì´ˆë§Œ ìˆëŠ” ê²½ìš°
                    seconds_list.append(float(ts))
            
            return seconds_list
        except Exception as e:
            print(f"âš ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± ì˜¤ë¥˜: {timestamp_str} - {e}")
            return []
    
    def calculate_average_timestamp(self, timestamps: List[float]) -> float:
        """timestamp ë¦¬ìŠ¤íŠ¸ì˜ í‰ê· ì„ ê³„ì‚°"""
        if not timestamps:
            return 0.0
        return np.mean(timestamps)
    
    def create_segments_parallel(self, original_video_path: str, comment_timestamp: float) -> List[str]:
        """ë³‘ë ¬ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (ìµœì í™”)"""
        segments = []
        center_time = comment_timestamp
        start_time = max(center_time - self.half_duration, 0)
        
        print(f"  ë³‘ë ¬ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±: {start_time}së¶€í„° {self.total_duration}ì´ˆ ({self.num_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜
        def create_single_segment(i):
            segment_start = start_time + (i * self.segment_duration)
            segment_end = segment_start + self.segment_duration
            
            base_name = os.path.splitext(os.path.basename(original_video_path))[0]
            segment_path = self.segments_dir / f"opt_segment_{base_name}_{int(comment_timestamp)}s_{i+1}_{segment_start:.1f}s_{segment_end:.1f}s.mp4"
            
            # ë³€ìˆ˜í™”ëœ ffmpeg ëª…ë ¹ì–´
            cmd = [
                "ffmpeg", "-i", original_video_path,
                "-ss", str(segment_start), "-t", str(self.segment_duration),
                "-c:v", "libx264", "-c:a", "aac", "-pix_fmt", "yuv420p",
                "-preset", self.ffmpeg_preset,
                "-crf", str(self.ffmpeg_crf),
            ]

            if self.ffmpeg_threads and self.ffmpeg_threads > 0:
                cmd.extend(["-threads", str(self.ffmpeg_threads)])

            cmd.extend([
                "-avoid_negative_ts", "make_zero",
                "-vsync", "cfr", "-r", str(self.video_fps),
                "-movflags", "+faststart",
                "-y", str(segment_path)
            ])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(segment_path):
                return (i, str(segment_path), True)
            else:
                return (i, None, False)
        
        # ë³‘ë ¬ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        segments = [None] * self.num_segments
        
        with ThreadPoolExecutor(max_workers=self.max_workers_segments) as executor:
            future_to_index = {executor.submit(create_single_segment, i): i for i in range(self.num_segments)}
            
            for future in as_completed(future_to_index):
                try:
                    index, segment_path, success = future.result()
                    segments[index] = segment_path
                    if success:
                        print(f"    âœ“ ì„¸ê·¸ë¨¼íŠ¸ {index+1}/{self.num_segments} ìƒì„± ì™„ë£Œ")
                    else:
                        print(f"    âœ— ì„¸ê·¸ë¨¼íŠ¸ {index+1}/{self.num_segments} ìƒì„± ì‹¤íŒ¨")
                except Exception as e:
                    index = future_to_index[future]
                    segments[index] = None
                    print(f"    âœ— ì„¸ê·¸ë¨¼íŠ¸ {index+1}/{self.num_segments} ì˜¤ë¥˜: {e}")
        
        return segments
    
    def create_segments_sequential(self, original_video_path: str, comment_timestamp: float) -> List[str]:
        """ìˆœì°¨ì ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (ë°±ì—…ìš©)"""
        segments = []
        center_time = comment_timestamp
        start_time = max(center_time - self.half_duration, 0)
        
        print(f"  ìˆœì°¨ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±: {start_time}së¶€í„° {self.total_duration}ì´ˆ ({self.num_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸)")
        
        for i in range(self.num_segments):
            segment_start = start_time + (i * self.segment_duration)
            segment_end = segment_start + self.segment_duration
            
            base_name = os.path.splitext(os.path.basename(original_video_path))[0]
            segment_path = self.segments_dir / f"opt_segment_{base_name}_{int(comment_timestamp)}s_{i+1}_{segment_start:.1f}s_{segment_end:.1f}s.mp4"
            
            # ë³€ìˆ˜í™”ëœ ffmpeg ëª…ë ¹ì–´
            cmd = [
                "ffmpeg", "-i", original_video_path,
                "-ss", str(segment_start), "-t", str(self.segment_duration),
                "-c:v", "libx264", "-c:a", "aac", "-pix_fmt", "yuv420p",
                "-preset", self.ffmpeg_preset,
                "-crf", str(self.ffmpeg_crf),
            ]

            if self.ffmpeg_threads and self.ffmpeg_threads > 0:
                cmd.extend(["-threads", str(self.ffmpeg_threads)])

            cmd.extend([
                "-avoid_negative_ts", "make_zero",
                "-vsync", "cfr", "-r", str(self.video_fps),
                "-movflags", "+faststart",
                "-y", str(segment_path)
            ])
            
            print(f"    ì„¸ê·¸ë¨¼íŠ¸ {i+1}/{self.num_segments} ìƒì„± ì¤‘...")
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(segment_path):
                segments.append(str(segment_path))
                print(f"    âœ“ ì„¸ê·¸ë¨¼íŠ¸ {i+1}/{self.num_segments} ìƒì„± ì™„ë£Œ")
            else:
                segments.append(None)
                print(f"    âœ— ì„¸ê·¸ë¨¼íŠ¸ {i+1}/{self.num_segments} ìƒì„± ì‹¤íŒ¨")
        
        return segments
    
    def extract_audio_optimized(self, video_path: str, start_time: float, 
                              duration: float = None) -> Tuple[np.ndarray, int]:
        """ìµœì í™”ëœ ì˜¤ë””ì˜¤ ì¶”ì¶œ"""
        if duration is None:
            duration = float(self.segment_duration)
            
        start_time = max(start_time, 0)
        duration = max(duration, 1)
        
        if not os.path.exists(video_path):
            return None, None
        
        base_name = os.path.splitext(os.path.basename(video_path))[0]
        temp_audio = f"/tmp/opt_temp_audio_{base_name}_{os.getpid()}.wav"
        
        # ë³€ìˆ˜í™”ëœ ì˜¤ë””ì˜¤ ì¶”ì¶œ ëª…ë ¹ì–´
        cmd_audio = [
            "ffmpeg", "-i", video_path, "-vn",
            "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
            "-f", "wav", "-y", temp_audio
        ]
        
        try:
            result = subprocess.run(cmd_audio, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(temp_audio):
                file_size = os.path.getsize(temp_audio)
                
                if file_size > 0:
                    with warnings.catch_warnings():
                        warnings.simplefilter("ignore")
                        audio, sr = sf.read(temp_audio)
                else:
                    # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ì— ë§ëŠ” ë¬´ìŒ ìƒì„±
                    audio = np.zeros(16000 * self.segment_duration, dtype=np.float32)
                    sr = 16000
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ì— ë§ëŠ” ë¬´ìŒ ìƒì„±
                audio = np.zeros(16000 * self.segment_duration, dtype=np.float32)
                sr = 16000
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if os.path.exists(temp_audio):
                os.remove(temp_audio)
            
            return audio, sr
            
        except Exception as e:
            if os.path.exists(temp_audio):
                os.remove(temp_audio)
            return None, None
    
    def process_segments_parallel(self, segment_paths: List[str], comment: str) -> List[str]:
        """ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ë™ì‹œ ì²˜ë¦¬"""
        print(f"  ë³‘ë ¬ ë¶„ì„ ëª¨ë“œ: {len(segment_paths)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        valid_segments = [(i, path) for i, path in enumerate(segment_paths) 
                         if path and os.path.exists(path)]
        
        if not valid_segments:
            return [f"No valid segments for processing"] * len(segment_paths)
        
        results = [""] * len(segment_paths)
        
        # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
        def process_single_segment_wrapper(orig_idx_and_path):
            orig_idx, segment_path = orig_idx_and_path
            try:
                result = self.process_single_segment_optimized(segment_path, orig_idx, comment)
                return (orig_idx, result)
            except Exception as e:
                return (orig_idx, f"Error in segment {orig_idx + 1}: {str(e)}")
        
        # ë³‘ë ¬ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
        with ThreadPoolExecutor(max_workers=self.max_workers_analysis) as executor:
            future_to_index = {executor.submit(process_single_segment_wrapper, segment): segment[0] 
                             for segment in valid_segments}
            
            for future in as_completed(future_to_index):
                try:
                    orig_idx, result = future.result()
                    results[orig_idx] = result
                    print(f"    âœ“ ì„¸ê·¸ë¨¼íŠ¸ {orig_idx+1} ë¶„ì„ ì™„ë£Œ")
                except Exception as e:
                    orig_idx = future_to_index[future]
                    results[orig_idx] = f"Error in segment {orig_idx + 1}: {str(e)}"
                    print(f"    âœ— ì„¸ê·¸ë¨¼íŠ¸ {orig_idx+1} ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        for i, path in enumerate(segment_paths):
            if not results[i]:
                results[i] = f"Failed to process segment {i + 1}"
        
        return results
    
    def process_segments_batch(self, segment_paths: List[str], comment: str) -> List[str]:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ì„¸ê·¸ë¨¼íŠ¸ ë™ì‹œ ì²˜ë¦¬ (ë°±ì—…ìš©)"""
        print(f"  ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ: {len(segment_paths)}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        # ìœ íš¨í•œ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        valid_segments = [(i, path) for i, path in enumerate(segment_paths) 
                         if path and os.path.exists(path)]
        
        if not valid_segments:
            return [f"No valid segments for processing"] * len(segment_paths)
        
        results = [""] * len(segment_paths)
        
        # ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬
        for batch_start in range(0, len(valid_segments), self.max_batch_size):
            batch_end = min(batch_start + self.max_batch_size, len(valid_segments))
            batch_segments = valid_segments[batch_start:batch_end]
            
            print(f"    ë°°ì¹˜ ì²˜ë¦¬ ì¤‘: {batch_start+1}-{batch_end}/{len(valid_segments)}")
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            conversations = []
            batch_indices = []
            
            for orig_idx, segment_path in batch_segments:
                batch_indices.append(orig_idx)
                
                # ì˜¤ë””ì˜¤ ì¶”ì¶œ
                audio, sr = self.extract_audio_optimized(segment_path, 0, duration=self.segment_duration)
                
                # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ë³€ìˆ˜í™”)
                system_prompt = f"""Analyze this {self.segment_duration}-second video segment and provide a description in exactly 2 parts:

VISUAL: Describe what you see in the video (objects, people, actions, scenes, colors, movements).
AUDIO: Describe what you hear in the audio (speech, music, sound effects, ambient sounds, silence).

Keep each part concise but informative."""
                
                user_content = []
                
                if os.path.exists(segment_path) and os.path.getsize(segment_path) > 0:
                    user_content.append({"type": "video", "video": segment_path})
                
                if audio is not None and len(audio) > 0 and not np.all(audio == 0):
                    user_content.append({"type": "audio", "audio": audio})
                
                user_content.append({"type": "text", "text": system_prompt})
                
                # Qwen ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì˜¤ë””ì˜¤ ì¶œë ¥ í˜¸í™˜ì„±)
                conversation = [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}],
                    },
                    {
                        "role": "user",
                        "content": user_content,
                    },
                ]
                
                conversations.append(conversation)
            
            # ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
            try:
                batch_results = self._process_batch_inference(conversations)
                
                for i, result in enumerate(batch_results):
                    if i < len(batch_indices):
                        results[batch_indices[i]] = result
                        
            except Exception as e:
                print(f"    ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                # ê°œë³„ ì²˜ë¦¬ë¡œ í´ë°±
                for i, (orig_idx, segment_path) in enumerate(batch_segments):
                    try:
                        result = self.process_single_segment_optimized(segment_path, orig_idx, comment)
                        results[orig_idx] = result
                    except Exception as e2:
                        results[orig_idx] = f"Error in segment {orig_idx + 1}: {str(e2)}"
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬
        for i, path in enumerate(segment_paths):
            if not results[i]:
                results[i] = f"Failed to process segment {i + 1}"
        
        return results
    
    def _process_batch_inference(self, conversations: List) -> List[str]:
        """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰"""
        if len(conversations) == 1:
            # ë‹¨ì¼ ì¶”ë¡ 
            return [self._single_inference(conversations[0])]
        
        # ì‹¤ì œ ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ ìˆœì°¨ ì²˜ë¦¬ë¡œ êµ¬í˜„
        # (Qwen2.5-OmniëŠ” ë°°ì¹˜ ì²˜ë¦¬ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŒ)
        results = []
        for conv in conversations:
            try:
                result = self._single_inference(conv)
                results.append(result)
            except Exception as e:
                results.append(f"Inference error: {str(e)}")
        
        return results
    
    def _single_inference(self, conversation) -> str:
        """ë‹¨ì¼ ì¶”ë¡  ì‹¤í–‰"""
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                text = self.processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
                
                audios, images, videos = process_mm_info(conversation, use_audio_in_video=self.USE_AUDIO_IN_VIDEO)
                
                inputs = self.processor(
                    text=text, 
                    audio=audios, 
                    images=images, 
                    videos=videos, 
                    return_tensors="pt", 
                    padding=True, 
                    use_audio_in_video=self.USE_AUDIO_IN_VIDEO
                )
            
            inputs = inputs.to(self.model.device).to(self.model.dtype)
            
            # ìµœì í™”ëœ ìƒì„±
            with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
                text_ids, audio = self.model.generate(**inputs, **self.generation_config,
                                                     use_audio_in_video=self.USE_AUDIO_IN_VIDEO)
            
            text = self.processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
            model_answer = text[0] if text else "Failed to generate response"
            
            # assistant ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "<|im_start|>assistant" in model_answer:
                assistant_start = model_answer.find("<|im_start|>assistant")
                assistant_end = model_answer.find("<|im_end|>", assistant_start)
                if assistant_end != -1:
                    model_answer = model_answer[assistant_start + len("<|im_start|>assistant"):assistant_end].strip()
                else:
                    model_answer = model_answer[assistant_start + len("<|im_start|>assistant"):].strip()
            elif "assistant" in model_answer:
                assistant_start = model_answer.find("assistant")
                model_answer = model_answer[assistant_start + len("assistant"):].strip()
                if model_answer.startswith(":"):
                    model_answer = model_answer[1:].strip()
            
            return model_answer
            
        except Exception as e:
            return f"Inference error: {str(e)}"
    
    def process_single_segment_optimized(self, segment_path: str, segment_index: int, comment: str) -> str:
        """ìµœì í™”ëœ ë‹¨ì¼ 6ì´ˆ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬"""
        try:
            if not segment_path or not os.path.exists(segment_path):
                return f"Failed to process segment {segment_index + 1}"
            
            audio, sr = self.extract_audio_optimized(segment_path, 0, duration=self.segment_duration)
            
            if audio is None or len(audio) == 0:
                return f"No audio in segment {segment_index + 1}"
            
            # ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ ë³€ìˆ˜í™”)
            system_prompt = f"""Analyze this {self.segment_duration}-second video segment and provide a brief description in exactly 2 parts:

VISUAL: Describe what you see in the video (objects, people, actions, scenes, colors, movements).
AUDIO: Describe what you hear in the audio (speech, music, sound effects, ambient sounds, silence).

Keep each part concise but informative."""
            
            user_content = []
            
            if os.path.exists(segment_path) and os.path.getsize(segment_path) > 0:
                user_content.append({"type": "video", "video": segment_path})
            
            if audio is not None and len(audio) > 0 and not np.all(audio == 0):
                user_content.append({"type": "audio", "audio": audio})
            
            user_content.append({"type": "text", "text": system_prompt})
            
            # Qwen ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì˜¤ë””ì˜¤ ì¶œë ¥ í˜¸í™˜ì„±)
            conversation = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."}],
                },
                {"role": "user", "content": user_content},
            ]
            
            return self._single_inference(conversation)
            
        except Exception as e:
            return f"Error in segment {segment_index + 1}: {str(e)}"
    
    def test_multiple_comments_optimized(self, num_comments: int = 20, video_test: bool = True, 
                                        use_parallel: bool = True, use_batch: bool = False) -> Dict[str, Any]:
        """ìµœì í™”ëœ ì—¬ëŸ¬ ëŒ“ê¸€ ì²˜ë¦¬"""
        try:
            df = pd.read_csv(os.path.join(PROJECT_ROOT, 'csv', 'merged_filtered_comments_with_dedup_lang.csv'))
            
            if len(df) == 0:
                return {"error": "Empty CSV file"}
            
            num_comments = min(num_comments, len(df))
            comments_to_process = df.head(num_comments)
            
            print(f"=== ìµœì í™”ëœ í…ŒìŠ¤íŠ¸: {num_comments}ê°œ ëŒ“ê¸€ ì²˜ë¦¬ ===")
            print(f"ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸: {'ON' if video_test else 'OFF'}")
            print(f"ë³‘ë ¬ ì²˜ë¦¬: {'ON' if use_parallel else 'OFF'}")
            print(f"ë°°ì¹˜ ì²˜ë¦¬: {'ON' if use_batch else 'OFF'}")
            print("=" * 50)
            
            all_results = []
            successful_comments = 0
            failed_comments = 0
            
            start_time = time.time()
            for idx, comment_row in tqdm(comments_to_process.iterrows(), total=num_comments, desc="ìµœì í™”ëœ ëŒ“ê¸€ ì²˜ë¦¬"):
                try:
                    video_id = comment_row['video_id']
                    comment = comment_row['comment']
                    language = comment_row['language']
                    
                    # ì§„í–‰ë¥  ì •ë³´
                    current_time = time.time()
                    elapsed_time = current_time - start_time
                    progress = (idx + 1) / num_comments
                    
                    if idx > 0:
                        avg_time_per_comment = elapsed_time / (idx + 1)
                        remaining_comments = num_comments - (idx + 1)
                        eta_seconds = avg_time_per_comment * remaining_comments
                        eta_str = f"{eta_seconds/60:.1f}ë¶„" if eta_seconds >= 60 else f"{eta_seconds:.0f}ì´ˆ"
                    else:
                        eta_str = "ê³„ì‚° ì¤‘..."
                    
                    print(f"\n[{idx+1}/{num_comments}] ğŸ“ ëŒ“ê¸€ ì²˜ë¦¬ ì¤‘... ({progress*100:.1f}%)")
                    print(f"â° ê²½ê³¼: {elapsed_time/60:.1f}ë¶„ | ETA: {eta_str}")
                    print(f"ğŸ¬ ë¹„ë””ì˜¤ ID: {video_id}")
                    print(f"ğŸ’¬ ëŒ“ê¸€: {comment[:80]}...")
                    print(f"ğŸŒ ì–¸ì–´: {language}")
                    
                    parsed_timestamps = self.parse_timestamp(comment_row['timestamp'])
                    avg_time = self.calculate_average_timestamp(parsed_timestamps)
                    
                    print(f"ğŸ“ íƒ€ì„ìŠ¤íƒ¬í”„: {comment_row['timestamp']} â†’ {avg_time:.1f}ì´ˆ")
                    
                    if avg_time <= 0:
                        print(f"  âŒ ì˜ëª»ëœ íƒ€ì„ìŠ¤íƒ¬í”„: {comment_row['timestamp']}")
                        failed_comments += 1
                        all_results.append({
                            "comment_index": idx + 1,
                            "video_id": video_id,
                            "comment": comment,
                            "language": language,
                            "error": "Invalid timestamp",
                            "analysis_type": "failed"
                        })
                        continue
                    
                    if not video_test:
                        result = {
                            "comment_index": idx + 1,
                            "video_id": video_id,
                            "comment": comment,
                            "language": language,
                            "comment_timestamp": avg_time,
                            "analysis_type": "text_only_test"
                        }
                        all_results.append(result)
                        successful_comments += 1
                        continue
                    
                    print(f"ğŸ” ë¹„ë””ì˜¤ íŒŒì¼ ê²€ìƒ‰ ì¤‘: {video_id}")
                    original_video_path = self.find_video_file(video_id)
                    if not original_video_path:
                        print(f"  âŒ ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_id}")
                        failed_comments += 1
                        all_results.append({
                            "comment_index": idx + 1,
                            "video_id": video_id,
                            "error": f"Video file not found: {video_id}",
                            "analysis_type": "failed"
                        })
                        continue
                    
                    print(f"  âœ… ë¹„ë””ì˜¤ íŒŒì¼ ë°œê²¬: {os.path.basename(original_video_path)}")
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (ë³‘ë ¬ ë˜ëŠ” ìˆœì°¨)
                    print(f"ğŸï¸  ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ì¤‘... ({self.num_segments}ê°œ Ã— {self.segment_duration}ì´ˆ)")
                    if use_parallel:
                        segment_paths = self.create_segments_parallel(original_video_path, avg_time)
                    else:
                        segment_paths = self.create_segments_sequential(original_video_path, avg_time)
                    
                    # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ (ë³‘ë ¬, ë°°ì¹˜, ë˜ëŠ” ìˆœì°¨)
                    print(f"ğŸ§  AI ë¶„ì„ ì‹œì‘...")
                    segment_start_time = time.time()
                    if use_parallel and not use_batch:
                        segment_queries_raw = self.process_segments_parallel(segment_paths, comment)
                    elif use_batch:
                        segment_queries_raw = self.process_segments_batch(segment_paths, comment)
                    else:
                        segment_queries_raw = []
                        for i, segment_path in enumerate(segment_paths):
                            if segment_path:
                                query = self.process_single_segment_optimized(segment_path, i, comment)
                            else:
                                query = f"Failed to process segment {i + 1}"
                            segment_queries_raw.append(query)
                    
                    segment_end_time = time.time()
                    total_segment_time = segment_end_time - segment_start_time
                    
                    # ê²°ê³¼ ì •ë¦¬ ë° ì¶œë ¥
                    segment_queries = []
                    print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
                    for i, query in enumerate(segment_queries_raw):
                        time_start = max(0, avg_time - self.half_duration + i * self.segment_duration)
                        time_end = max(0, avg_time - self.half_duration + (i + 1) * self.segment_duration)
                        time_range = f"{time_start:.1f}s - {time_end:.1f}s"
                        
                        segment_queries.append({
                            "segment_index": i + 1,
                            "time_range": time_range,
                            "query": query
                        })
                        
                        print(f"  {i+1}. [{time_range}]: {query[:100]}...")
                    
                    result = {
                        "comment_index": idx + 1,
                        "video_id": video_id,
                        "comment": comment,
                        "language": language,
                        "comment_timestamp": avg_time,
                        "total_duration": f"{self.num_segments * self.segment_duration}s ({self.num_segments} segments of {self.segment_duration}s each)",
                        "segments": segment_queries,
                        "analysis_type": "optimized_video_analysis",
                        "processing_time": total_segment_time,
                        "optimization": {
                            "parallel_processing": use_parallel,
                            "batch_processing": use_batch,
                            "model_compiled": True
                        }
                    }
                    
                    all_results.append(result)
                    successful_comments += 1
                    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {total_segment_time:.2f}ì´ˆ")
                    print(f"âœ… ëŒ“ê¸€ {idx+1} ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
                    print("=" * 60)
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    if idx % 5 == 0:  # 5ê°œ ëŒ“ê¸€ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                        torch.cuda.empty_cache()
                        gc.collect()
                    
                except Exception as e:
                    print(f"  âŒ ëŒ“ê¸€ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    failed_comments += 1
                    all_results.append({
                        "comment_index": idx + 1,
                        "error": f"Processing failed: {str(e)}",
                        "analysis_type": "failed"
                    })
            
            final_result = {
                "total_comments": num_comments,
                "successful_comments": successful_comments,
                "failed_comments": failed_comments,
                "success_rate": f"{(successful_comments/num_comments)*100:.1f}%" if num_comments > 0 else "0%",
                "analysis_type": "optimized_multiple_comments_test",
                "optimization_features": {
                    "model_compilation": True,
                    "parallel_processing": use_parallel,
                    "batch_processing": use_batch,
                    "memory_management": True,
                    "float16_precision": True,
                    "optimized_generation_config": True
                },
                "comments": all_results
            }
            
            print(f"\n=== ìµœì í™”ëœ ì²˜ë¦¬ ì™„ë£Œ ===")
            print(f"ì´ ëŒ“ê¸€: {num_comments}ê°œ")
            print(f"ì„±ê³µ: {successful_comments}ê°œ")
            print(f"ì‹¤íŒ¨: {failed_comments}ê°œ")
            print(f"ì„±ê³µë¥ : {(successful_comments/num_comments)*100:.1f}%")
            
            return final_result
            
        except Exception as e:
            print(f"ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return {"error": f"Optimized test failed: {str(e)}"}

def main():
    print("=== ìµœì í™”ëœ í…ŒìŠ¤íŠ¸ìš© Moment Query Generator ===")
    print("20ê°œ ëŒ“ê¸€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìµœì í™” ë²„ì „)")
    print("=" * 50)
    
    total_start_time = time.time()
    
    # ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™” (ëª¨ë“  ì„¤ì • ë³€ìˆ˜í™”)
    print("\nìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model_start_time = time.time()
    generator = OptimizedTestMomentQueryGenerator(
        # ëª¨ë¸ ì„¤ì •
        model_path="Qwen/Qwen2.5-Omni-7B",
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì„¤ì • (ìœ ì—°í•˜ê²Œ ì¡°ì • ê°€ëŠ¥)
        segment_duration=6,
        num_segments=3,  # 3ê°œë¡œ ì„¤ì • (18ì´ˆ ì´ ê¸¸ì´, ì¤‘ì‹¬ Â±9ì´ˆ)
        
        # ë°°ì¹˜ ë° ìƒì„± ì„¤ì •
        max_batch_size=3,
        max_new_tokens=200,
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬ ì„¤ì •
        video_fps=24,
        ffmpeg_preset="ultrafast",  # ê°€ì¥ ë¹ ë¥¸ ì„¤ì •
        ffmpeg_crf=28,  # ì ë‹¹í•œ í’ˆì§ˆ
        
        # ìµœì í™” ê¸°ëŠ¥ ì„¤ì •
        use_model_compile=True,
        use_flash_attention=True,
        torch_dtype_str="float16",
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        max_workers_segments=3,  # ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± ë³‘ë ¬ë„
        max_workers_analysis=2   # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ë³‘ë ¬ë„
    )
    model_end_time = time.time()
    model_time = model_end_time - model_start_time
    print(f"âœ“ ìµœì í™”ëœ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ ({model_time:.2f}ì´ˆ)")
    
    # ìµœì í™”ëœ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ í¬í•¨)
    print("\nìµœì í™”ëœ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (20ê°œ ëŒ“ê¸€)...")
    total_duration = generator.num_segments * generator.segment_duration
    print(f"   ìµœì í™” ê¸°ëŠ¥: ëª¨ë¸ ì»´íŒŒì¼, ë³‘ë ¬ ì²˜ë¦¬, {generator.num_segments}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ({total_duration}ì´ˆ), ë©”ëª¨ë¦¬ ê´€ë¦¬")
    print(f"   ë³‘ë ¬ ì„¤ì •: ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± {generator.max_workers_segments}ê°œ, ë¶„ì„ {generator.max_workers_analysis}ê°œ")
    print("   ì˜ˆìƒ ì‹œê°„: ê¸°ì¡´ ëŒ€ë¹„ 70-85% ë‹¨ì¶• ì˜ˆìƒ (ë³‘ë ¬í™” + ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ê°ì†Œ + ê¸°íƒ€ ìµœì í™”)")
    
    video_start_time = time.time()
    video_result = generator.test_multiple_comments_optimized(
        num_comments=20, 
        video_test=True, 
        use_parallel=True,  # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
        use_batch=False     # ë³‘ë ¬ì´ ë” íš¨ìœ¨ì ì´ë¯€ë¡œ ë°°ì¹˜ëŠ” ë¹„í™œì„±í™”
    )
    video_end_time = time.time()
    video_time = video_end_time - video_start_time
    
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    if "error" in video_result:
        print(f"ìµœì í™”ëœ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {video_result['error']} ({video_time:.2f}ì´ˆ)")
    else:
        print(f"âœ“ ìµœì í™”ëœ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ({video_time:.2f}ì´ˆ)")
        print(f"  ì´ ëŒ“ê¸€: {video_result['total_comments']}ê°œ")
        print(f"  ì„±ê³µ: {video_result['successful_comments']}ê°œ")
        print(f"  ì‹¤íŒ¨: {video_result['failed_comments']}ê°œ")
        print(f"  ì„±ê³µë¥ : {video_result['success_rate']}")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìµœì í™”ëœ ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")
    if "error" not in video_result:
        print(f"ì´ ëŒ“ê¸€: {video_result['total_comments']}ê°œ")
        print(f"ì„±ê³µ: {video_result['successful_comments']}ê°œ")
        print(f"ì‹¤íŒ¨: {video_result['failed_comments']}ê°œ")
        print(f"ì„±ê³µë¥ : {video_result['success_rate']}")
        
        # ìµœì í™” ê¸°ëŠ¥ ìš”ì•½
        print(f"\nì ìš©ëœ ìµœì í™”:")
        opt_features = video_result.get('optimization_features', {})
        for feature, enabled in opt_features.items():
            print(f"  - {feature}: {'âœ“' if enabled else 'âœ—'}")
    
    # ì‹œê°„ í†µê³„
    print(f"\n=== ìµœì í™”ëœ ì‹¤í–‰ ì‹œê°„ í†µê³„ ===")
    print(f"ëª¨ë¸ ì´ˆê¸°í™”: {model_time:.2f}ì´ˆ")
    print(f"ìµœì í™”ëœ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸: {video_time:.2f}ì´ˆ")
    print(f"ì´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.1f}ë¶„)")
    
    if "error" not in video_result and video_result.get('successful_comments', 0) > 0:
        avg_time_per_comment = video_time / video_result['successful_comments']
        print(f"ëŒ“ê¸€ë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time_per_comment:.2f}ì´ˆ")
        print(f"ì˜ˆìƒ ì†ë„ í–¥ìƒ: ê¸°ì¡´ ëŒ€ë¹„ ì•½ 2-3ë°° ë¹ ë¦„")
    
    # ê²°ê³¼ ì €ì¥
    video_result['timing'] = {
        "model_initialization": model_time,
        "optimized_video_test": video_time,
        "total_time": total_time,
        "avg_time_per_comment": video_time / video_result.get('successful_comments', 1) if video_result.get('successful_comments', 0) > 0 else 0
    }
    
    output_file = "optimized_test_moment_query_result_20comments.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(video_result, f, ensure_ascii=False, indent=2)
    print(f"\nìµœì í™”ëœ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
